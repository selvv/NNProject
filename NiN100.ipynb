{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 23 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 23 days\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "from scipy.ndimage.filters import convolve\n",
    "\n",
    "#note: this requires the starter code for the assignments!\n",
    "from common.plotting import plot_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample\n",
    "import lasagne\n",
    "import time,colorsys\n",
    "from skimage.transform import rotate\n",
    "print theano.config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.stdout,TMP = open(\"lab110-01-U7.txt\",\"w\"),sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "cifar_train = CIFAR10((\"train\",), subset=slice(None,40000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "cifar_train_stream = DataStream.default_stream(\n",
    "    cifar_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar_train.num_examples, 25))\n",
    "                                               \n",
    "cifar_validation = CIFAR10((\"train\",), subset=slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "cifar_validation_stream = DataStream.default_stream(\n",
    "    cifar_validation, iteration_scheme=SequentialScheme(cifar_validation.num_examples, 25))\n",
    "cifar_test = CIFAR10((\"test\",))\n",
    "cifar_test_stream = DataStream.default_stream(\n",
    "    cifar_test, iteration_scheme=SequentialScheme(cifar_test.num_examples, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#based on https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/merge.py\n",
    "class RandomChoseLayer(lasagne.layers.ElemwiseMergeLayer):\n",
    "    def __init__(self, incomings, cropping=None, **kwargs):\n",
    "        super(RandomChoseLayer, self).__init__(incomings, theano.tensor.add, **kwargs)\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        self._srng = RandomStreams(lasagne.random.get_rng().randint(1, 2147462579))\n",
    "        x = self._srng.uniform(size=(1,), low=0.0, high=len(inputs))\n",
    "        L = [(i <= x)*(x < i+1)  for i in range(len(inputs))]\n",
    "        inputs = [input * coeff if coeff != 1 else input\n",
    "                  for coeff, input in zip(L, inputs)]\n",
    "\n",
    "        # pass scaled inputs to the super class for summing\n",
    "        return super(RandomChoseLayer, self).get_output_for(inputs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RandomSliceLayer(input,size):\n",
    "    L = []\n",
    "    a,b = lasagne.layers.get_output_shape(input)[-2:]\n",
    "    for x in range(0,a-size):\n",
    "        l1 = lasagne.layers.SliceLayer(input, indices=slice(x,x+size), axis=-1)\n",
    "        for y in range(0,b-size):\n",
    "            l2 = lasagne.layers.SliceLayer(l1, indices=slice(y,y+size), axis=-2)\n",
    "            L.append(l2)\n",
    "    return RandomChoseLayer(L)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_deepCNiN(input_var=None, l = 5, k = 100, drop = [0,0,0.1,0.2,0.3,0.4,0.5]):\n",
    "\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 3, 96, 96),\n",
    "                                        input_var=input_var)\n",
    "\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            lasagne.layers.dropout(network, p=drop[0]),\n",
    "            num_filters=k, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    for i in range(l):\n",
    "        network = lasagne.layers.MaxPool2DLayer(network,pool_size=(2,2))\n",
    "        network = lasagne.layers.NINLayer(network,num_units=k*(i+1))\n",
    "        network = lasagne.layers.Conv2DLayer(\n",
    "            lasagne.layers.dropout(network, p=drop[i+1]),\n",
    "            num_filters=k*(i+2), filter_size=(2, 2),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    network = lasagne.layers.NINLayer(network,num_units=k*(l+1))\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=drop[-1]),\n",
    "            num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RGBtoYUV(R,G,B):\n",
    "    Y = 0.299 * R + 0.587 * G  + 0.114 * B\n",
    "    U = 0.492 * (B - Y)\n",
    "    V = 0.877 * (R - Y)\n",
    "    return Y,U,V\n",
    "\n",
    "def model_change(picture,function=colorsys.rgb_to_hsv):\n",
    "    picture2 = zeros(shape = shape(picture))\n",
    "    for i in range(shape(picture)[1]):\n",
    "        for j in range(shape(picture)[2]):\n",
    "            a,b,c = picture[:,i,j]\n",
    "            a,b,c = function((a+1)/2,(b+1)/2,(c+1)/2)\n",
    "            picture2[:,i,j] = a*2-1,b*2-1,c*2-1\n",
    "    return picture2\n",
    "\n",
    "def DataTransform(inputs,outputs,num=1):\n",
    "    #return inputs,outputs\n",
    "    a,b = (96,96)\n",
    "    inp = []\n",
    "    out = []\n",
    "    \n",
    "    for x,y in zip(inputs,outputs):\n",
    "        for i in range(num):\n",
    "            _,sa,sb = shape(x)\n",
    "            rx,ry = random.randint(-16,16)+sa,random.randint(-16,16)+sb\n",
    "            xx = zeros(shape=(3,a,b),dtype = float32)\n",
    "            xx[:,rx:rx+sa,ry:ry+sb] = x\n",
    "            ang = random.randint(-15,15)\n",
    "            for i in range(3):\n",
    "                xx[i] = rotate(xx[i], ang)\n",
    "            inp.append(xx)\n",
    "            out.append(y)\n",
    "            #inp.append(model_change(xx,function=RGBtoYUV))\n",
    "            #out.append(y)\n",
    "    return inp,out\n",
    "\n",
    "\n",
    "for inputs, targets in cifar_train_stream.get_epoch_iterator():\n",
    "    A,B = DataTransform(inputs,targets)\n",
    "    break\n",
    "    for S in A:\n",
    "        aa,bb,cc = shape(S)\n",
    "        S = S.reshape((1,aa,bb,cc))\n",
    "        plot_mat(S.transpose(1,0,2,3), cmap='gray')\n",
    "        show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print 'Nets:', len(list_of_nets)\n",
    "except:\n",
    "    print 'new list'\n",
    "    list_of_nets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snapshot(net):\n",
    "    return lasagne.layers.get_all_param_values(net)\n",
    "def restore(net, params):\n",
    "    lasagne.layers.set_all_param_values(net,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = theano.tensor.tensor4('inputs')\n",
    "target_var = theano.tensor.ivector('targets')\n",
    "\n",
    "net = build_deepCNiN(input_var)\n",
    "\n",
    "prediction = lasagne.layers.get_output(net)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "l2_penalty = lasagne.regularization.regularize_layer_params(lasagne.layers.get_all_layers(net), lasagne.regularization.l2)\n",
    "weight_penalty = theano.tensor.scalar('weight_penalty')\n",
    "loss = loss.mean()#+weight_penalty*l2_penalty\n",
    "\n",
    "l_rate = theano.tensor.scalar('l_rate')\n",
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "    loss, params, learning_rate=l_rate, momentum=0.75)\n",
    "test_prediction = lasagne.layers.get_output(net, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = theano.tensor.mean(theano.tensor.eq(theano.tensor.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var,l_rate], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "num_epochs = 51\n",
    "i = 0\n",
    "weight_penalty_value = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for inputs, targets in cifar_train_stream.get_epoch_iterator():\n",
    "        in4,ou4 = DataTransform(inputs, targets[:,0],2)\n",
    "        i += 1\n",
    "        K = 10000\n",
    "        lrate = 3e-2 * K / np.maximum(K, i)\n",
    "        train_err += train_fn(in4, ou4,lrate)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for inputs, targets in cifar_validation_stream.get_epoch_iterator():\n",
    "        in4,ou4 = DataTransform(inputs, targets[:,0])\n",
    "        err, acc = val_fn(in4, ou4)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    if epoch % 10 == 0:\n",
    "        test_err = 0\n",
    "        test_acc = 0\n",
    "        test_batches = 0\n",
    "        for inputs, targets in cifar_train_stream.get_epoch_iterator():\n",
    "            in4,ou4 = DataTransform(inputs, targets[:,0])\n",
    "            err, acc = val_fn(in4, ou4)\n",
    "            test_err += err\n",
    "            test_acc += acc\n",
    "            test_batches += 1\n",
    "        print(\"Final results:\")\n",
    "        print(\"  train loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "        print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "                test_acc / test_batches * 100))\n",
    "        if test_acc/test_batches - val_acc/val_batches > 0.09 and weight_penalty_value < 0.005:\n",
    "            weight_penalty_value += 0.0005\n",
    "        print 'weight_penalty_value:', weight_penalty_value\n",
    "    sys.stdout.flush()\n",
    "\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for inputs, targets in cifar_train_stream.get_epoch_iterator():\n",
    "    in4,ou4 = DataTransform(inputs, targets[:,0])\n",
    "    err, acc = val_fn(in4, ou4)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  train loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))    \n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for inputs, targets in cifar_validation_stream.get_epoch_iterator():\n",
    "    in4,ou4 = DataTransform(inputs, targets[:,0])\n",
    "    err, acc = val_fn(in4, ou4)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "list_of_nets.append((test_acc / test_batches * 100,snapshot(net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TMP,sys.stdout = sys.stdout,TMP\n",
    "TMP.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_fn = theano.function([input_var], argmax(prediction,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_result = []\n",
    "for inputs, targets in cifar_test_stream.get_epoch_iterator():\n",
    "    in4,ou4 = DataTransform(inputs, targets[:,0])\n",
    "    pre = pred_fn(in4)\n",
    "    test_result += zip(pre,ou4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = open('result_NiN_100','w')\n",
    "for (x,y) in test_result:\n",
    "    st.write('%d %d\\n'%(x,y))\n",
    "st.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
